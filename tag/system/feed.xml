<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator>
  <link href="/tag/system/feed.xml" rel="self" type="application/atom+xml" />
  <link href="/" rel="alternate" type="text/html" />
  <updated>2023-11-02T18:07:22+01:00</updated>
  <id>/tag/system/feed.xml</id>

  
  
  

  
    <title type="html">Özgün ÖZ | </title>
  

  
    <subtitle>Fullstack Developer, Msc. Computer Engineer</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">Clickhouse 101 - Data Structures and internals</title>
      <link href="/clickhouse101_1" rel="alternate" type="text/html" title="Clickhouse 101 - Data Structures and internals" />
      <published>2023-08-30T12:18:00+02:00</published>
      <updated>2023-08-30T12:18:00+02:00</updated>
      <id>/clickhouse101_1</id>
      <content type="html" xml:base="/clickhouse101_1">&lt;p&gt;First article of the clickhouse 101 series. Lets deep dive into the underlying data structures and internal mechanisms of clickhouse.&lt;/p&gt;

&lt;p&gt;Clickhouse is a DB that has a lot of potential and growing fast. Its use cases increase very rapidly as well. If you are into Data Analytics and Bigdata, you must explore it before too late.
This series of articles are meant to give an easy introduction to clickhouse to mainly junior level engineers, but you will still find interesting content if you are more experienced. 
The official documentation is evolving very fast. It is now complete in therms of content, and it is easier to read and browse through.&lt;br /&gt;
So in this series we will be covering topics necessary knowledge before you dig into the official documentation, a résumé of most important points that differ clickhouse and that need attention, lastly and most importantly give experience based and neutral recommendations.&lt;/p&gt;

&lt;p&gt;The series of articles will be as the following:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;/clickhouse101_1&quot;&gt;Clickhouse DataStructures and Internals&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Modeling your data in Clickhouse&lt;/li&gt;
  &lt;li&gt;Tradeoffs to take into account for Optimization&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;In this first article, we will cover a bit of theory before getting into the practice. We will refresh our memory on the basics of DBs in general and the datastructures used in them. Lastly see how these structures are used in clickhouse and where does its power come from. &lt;br /&gt;
We will cover the following topics:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#whatisCH&quot; id=&quot;whatisCHref&quot;&gt;What is Clickhouse ?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#dsCH&quot; id=&quot;dsCHref&quot;&gt;Datastructures used in DBs and in Clickhouse&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#internalCH&quot; id=&quot;internalCHref&quot;&gt;Clickhouse internal Storage and Query Mechanisms&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;whatisCH&quot;&gt; 1. What is Clickhouse ? &lt;/h1&gt;

&lt;p&gt;The best answer to that comes always from the &lt;a href=&quot;https://clickhouse.com/docs/en/intro&amp;quot;&quot;&gt;official documentation&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
ClickHouse® is a high-performance, column-oriented SQL database management system (DBMS) for online analytical processing (OLAP). It is available as both an open-source software and a cloud offering.
&lt;/blockquote&gt;

&lt;p&gt;To describe its differences briefly, before we dig into the details of each of them:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It is a true Column Based Database&lt;/li&gt;
  &lt;li&gt;Uses data compression as a key role of its internal mechanism&lt;/li&gt;
  &lt;li&gt;Stores data on disk&lt;/li&gt;
  &lt;li&gt;Process data in parallel (CPU) and has a distributed architecture (every instance is independent)&lt;/li&gt;
  &lt;li&gt;Has SQL support (pretty much)&lt;/li&gt;
  &lt;li&gt;Real time Data inserts (async data inserts do not block queries)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In order to understand it’s true power, its difference from other DBs, now let’s try to understand the first 3 points listed above and the basic concepts behind DBs in general and Clickhouse.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;dsCH&quot;&gt; 2. Datastructures used in DBs and in Clickhouse&lt;/h1&gt;
&lt;h2&gt; 2.1. How Row and Column based Databases work&lt;/h2&gt;

&lt;p&gt;Lets say we have the following DB table.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/mastering_clickhouse-DB table.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Row oriented DBs, as the name suggest, store data in rows. It is easy to write and read entire rows. But not so much if you want to read adhoc Query or do aggregation on particular columns, like analytical queries does. 
In that case you have to bring all columns for all rows, read multiple disks potentially, then filter on columns you are interested, then only aggregate.&lt;/p&gt;

&lt;p&gt;For example if we want to add a new row in a Row oriented DB, we would be appending the data like below.&lt;br /&gt;
Reading from it would require reading all columns.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/mastering_clickhouse-Row%20based%20DBs.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now imagine if we just wanted to sum the ages in this table. We would have to read all data columns first to get the data we need. Also if we partition the table into multiple disks, this would mean that the computer would need to read from multiple disks to get the data necessary.&lt;/p&gt;

&lt;p&gt;Now lets compare the same scenario for the same data stored on a Column Oriented design.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/mastering_clickhouse-Column%20based%20DBs.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we want to add new data, we would have to plug each column’s data into where it belongs. If we use a single disk, we would have to bring everything in memory to do this. But it is the same with Row Oriented design. Column Oriented design has its advantage if we partition data into multiple disks.&lt;/p&gt;

&lt;p&gt;As we can see from above, when reading a single column for aggregating on it, we just need one disk, and the data loaded in memory is much less.&lt;/p&gt;

&lt;p&gt;Now, If we want to save space on our DB and secure data, we may consider data encoding and compression. So lets see what that means.&lt;/p&gt;

&lt;h2&gt; 2.2. Compression ? Encoding ? Encryption ? Hashing ? Indexing ? Huh ?!&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/compression_definition.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;As the description above says, compression is a type of encoding. So lets start with that first.&lt;/p&gt;

&lt;h3&gt; What is Encoding ? &lt;/h3&gt;

&lt;p&gt;Encoding data is a process involving changing data into a new format using a scheme. Encoding is a reversible process and data can be encoded to a new format and decoded to its original format. Encoding typically involves a publicly available scheme that is easily reversed. Encoding data is typically used to ensure the integrity and usability of data and is commonly used when data cannot be transferred in its current format between systems or applications. Encoding is not used to protect or secure data because it is easy to reverse.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;An example of encoding - Base64  &lt;/strong&gt;
It is a method to encode byte sequences to a string. Also known as ASCII encoding, it converts binary data to ASCII strings.
But it is not the ideal choice for security as it can easily be decoded.
Instead, it serves as an easy way to make non HTTP compatible data types readable (image, audio ect…)
We can attach a base64 encoded image into an xml or email.
It provides no checksum or anything for storage value, so it is really used for transport.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; Another example of encoding - UTF &lt;/strong&gt;
It is a method used for storage. 
UTF Stands for “Unicode Transformation Format”. The UTF encoding standards, such as UTF-8, 16 ect., are used to convert Unicode character into numerals. &lt;strong&gt;Unicode&lt;/strong&gt; is a codded character set. A set of characters and mapping between those characters and their integer codes representing them.&lt;/p&gt;

&lt;p&gt;Lets say we want to store the following character in our hard drive.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;character:              汉
unicode:                U+6C49
unicode in binary:      01101100  01001001&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;When we decide to store a character in our hard drive, we can simply put its binary value. When the computer reads it, it has no idea how to parse this. It is one byte 2 charcter ? or two bytes 1 character ? so we need an encoding to tell the computer how to treat it when read. This is where UTF-8 like encodings comes in.&lt;/p&gt;

&lt;p&gt;Binary Format of Byte Sequences in UTF-8 are like the following:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../assets/images/mastering_clickhouse-utf-table.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Which format from the above table we will use depends on how many bits we need to write in binary the unicode of our character (for 汉 we need 2bytes/16bits).  &lt;br /&gt;
So to Encode 汉 in UTF-8 we will use the 3bytes format which has 16bits free:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../assets/images/mastering_clickhouse-utf-encoding.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Encoding of 汉 in UTF-8   =  11100110 10110001 10001001&lt;/p&gt;

&lt;h3&gt; What is Encryption ? &lt;/h3&gt;

&lt;p&gt;Encryption is an encoding technique for a specific need, which is to allow only authorized users with a key or password to decrypt the data and reveal its original. Encryption is used when data needs to be protected so those without the decryption keys cannot access the original data. When data is sent to a website over HTTPS it is encrypted using the public key type. While encryption does involve encoding data, the two are not interchangeable terms, encryption is always used when referring to data that has been securely encoded. Encoding data is used only when talking about data that is not securely encoded.&lt;/p&gt;

&lt;p&gt;There are two basic types of encryption: symmetric key and public key.
In a symmetric key, the same key is used to encrypt and decrypt data, like a password. In public key encryption, one key is used to encrypt data and a different key is used to decrypt the data.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;An example of encryption is: AES 256 &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;AES is the Advanced Encryption Standard and is a symmetric key encryption. AES uses a 256-bit key which means there are 2^256, or 1.158x10^77, possible keys that can be used.
Clickhouse allows, if needed, encrypting the data at rest with an AES key.&lt;/p&gt;

&lt;h3&gt; What is Hashing ? &lt;/h3&gt;
&lt;p&gt;Hashing is a one-way process where data is encoded, using a hash-function.
The best definition of a hash function comes from &lt;a href=&quot;&amp;quot;https://en.wikipedia.org/wiki/Hash_function&amp;quot;&quot;&gt;wikipedia&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
A hash function is any function that can be used to map data of arbitrary size to fixed-size values, though there are some hash functions that support variable length output. The values returned by a hash function are called hash values, hash codes, digests, or simply hashes. The values are usually used to index a fixed-size table called a hash table. Use of a hash function to index a hash table is called hashing or scatter storage addressing.
&lt;/blockquote&gt;

&lt;p&gt;As we can understand from that definition, hashing can be using to index data in Databases, which we will see in a bit. But on top of that Hashing is also commonly used to verify the integrity of data, commonly referred to as a checksum. If two pieces of identical data are hashed using the same hash function, the resulting hash will be identical. If the two pieces of data are different, the resulting hashes will be different and unique.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; An example of a hash function - SHA512 &lt;/strong&gt;
Say Alice wants to send Bob a file and verify that Bob has the exact same file and that no changes occurred in the transferring process. Alice will email Bob the file along with a hash of the file. After Bob downloads the file, he can verify the file is identical by performing a hash function on the file and verify the resulting hash is the same as Alice provided.&lt;/p&gt;

&lt;h3&gt; What is Compression ? &lt;/h3&gt;

&lt;p&gt;Compression mechanisms seperates into two, losseless and lossy. As the names suggest, lossless types can compress/decompress data and reconstruct it without any data loss. ex: zip archives which includes tarball(tar.xz) compression for unix systems.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt; An example of Compression - Dictionary Encoding:&lt;/strong&gt;
Most primitive, yet most powerful encoding. It compresses really good, but the dictionary built is usually for a specific purpose. Otherwise too big an not optimal. A common usage is Brotli algorithm used to compress web pages.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;key&lt;/th&gt;
      &lt;th&gt;encoded value&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;pizza&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;spageti&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;döner&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;An example of Compression - Bitmap Compression / Run-length Encoding:&lt;/strong&gt;
It is a form of losseless compression in which “runs” of data (sequences in which the same data value occurs in many consecutive data elements) are stored as a single data value and count.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;aaaab -&amp;gt; 4a1b&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h2&gt; 2.3. Why Compression and Encoding is important in C-Oriented DBs ? &lt;/h2&gt;
&lt;p&gt;Now if we get back to our discussion about R-DBs and C-DBs, in columnar design we can sort column data, and compress with run-lenght encoding. Also since we encode data, each piece of data is the same number of bits long. So we can further compress, so each data piece is shown as, the number of peice of data times the number of bits each data piece has.&lt;/p&gt;

&lt;p&gt;When doing adHoc queries, there are different sort orders of data that would improve performance. We might want to list data ordered by date, for instance both in asc and desc order. In R-DBs, indexes can be created but data is rarely ordered with multiple sort orders. In C-DBs you can.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../assets/images/mastering_clickhouse-ColumnOriented-ReadStores.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These ways of storing data are called “projections”.
There are multiple benefits beyond query performance. Having multiple copies of same data allows fault tolerance.&lt;/p&gt;

&lt;p&gt;The above diagram seems difficult to update, and it is. But that is why usually C-DBs have the main table called “writable store (WS)” and multiple “readable stores (RS)”.
The WS has data ordered in order of injection. It simply appends new data to the existing one. It has a tuple mover that updates the RS with the updates to the WS.
The RS can have multiple projections. The tuple mover navigates to the projections and append data to the proper place.&lt;/p&gt;
&lt;div class=&quot;alert&quot;&gt;
    &lt;div style=&quot;margin-left: 0.7em;&quot;&gt;
        &lt;!--&lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; width=&quot;16&quot; height=&quot;16&quot; fill=&quot;currentColor&quot; class=&quot;bi bi-exclamation-triangle-fill&quot; viewBox=&quot;0 0 16 16&quot;&gt;
            &lt;path d=&quot;M8.982 1.566a1.13 1.13 0 0 0-1.96 0L.165 13.233c-.457.778.091 1.767.98 1.767h13.713c.889 0 1.438-.99.98-1.767L8.982 1.566zM8 5c.535 0 .954.462.9.995l-.35 3.507a.552.552 0 0 1-1.1 0L7.1 5.995A.905.905 0 0 1 8 5zm.002 6a1 1 0 1 1 0 2 1 1 0 0 1 0-2z&quot;/&gt;
        &lt;/svg&gt;--&gt; 
        Warning:
        &lt;em&gt;This architecture requires that the partially inserted data to the RS should be ignored by the incoming adhoc queries, untill the insertion is completed&lt;/em&gt;
    &lt;/div&gt;
&lt;/div&gt;

&lt;h3&gt;Indexing&lt;/h3&gt;
&lt;p&gt;Now as last thing before we get to the implementations in ClickHouse, some insights on the indexing mechanisms.
Before we see indexing in DBs, I would like to state the definition of the word &lt;a href=&quot;https://en.wikipedia.org/wiki/Index_(publishing)&quot;&gt;“index”&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;
An index is a list of words or phrases (&apos;headings&apos;) and associated pointers (&apos;locators&apos;) to where useful material relating to that heading can be found in a document or collection of documents. Examples are an index in the back matter of a book and an index that serves as a library catalog. An index differs from a word index, or concordance, in focusing on the subject of the text rather than the exact words in a text, and it differs from a table of contents because the index is ordered by subject, regardless of whether it is early or late in the book, while the listed items in a table of contents is placed in the same order as the book.[1]
&lt;/blockquote&gt;

&lt;p&gt;Now we can understand deeply its usage &lt;a href=&quot;https://en.wikipedia.org/wiki/Database_index&quot;&gt;in DBs&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;
A database index is a data structure that improves the speed of data retrieval operations on a database table at the cost of additional writes and storage space to maintain the index data structure.
&lt;/blockquote&gt;

&lt;p&gt;Indexes in DBs are created using the following two info:&lt;br /&gt;
&lt;strong&gt;Search key:&lt;/strong&gt; A copy of the primary key, the values of the search key is stored in sorted order to be able to search on them easily.&lt;br /&gt;
&lt;strong&gt;Data ref:&lt;/strong&gt; or pointer, which contains addresses of disk block where the value row of that particular key is stored.    &lt;br /&gt;
&lt;em&gt;NOTE: the pointed data itself may or may not be ordered.&lt;/em&gt;&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;key&lt;/th&gt;
      &lt;th&gt;DataRef&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0x012F&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;0xA29E&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;0x27D2&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;…&lt;/td&gt;
      &lt;td&gt;…&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;There exists two types of index file organisations:&lt;br /&gt;
&lt;strong&gt; 1. Sequential file organisation &lt;/strong&gt;&lt;br /&gt;
&lt;strong&gt; 2. Hash file organisation &lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Lets illustrate how those organisations work using the following DB table. The table is spread across 2 pages that contain four rows each.&lt;br /&gt;
&lt;strong&gt;Partition ?&lt;/strong&gt; Partitioning allows a table, index, or index-organized table to be subdivided into smaller pieces, where each piece of such a database object is called a partition. Each partition has its own name, and may optionally have its own storage characteristics. The data of partitioned tables and indexes is divided into units that can be spread across more than one filegroup in a database. The data is partitioned horizontally, so that groups of rows are mapped into individual partitions.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../assets/images/mastering_clickhouse-indexing-table.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We will start with Hash file, and finish with Sequential file index, cause the latest is the one used in ClickHouse.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hash file organisation:&lt;/strong&gt;
Is an index that uses a hash function with search keys as parameters to generate addresses of data record.
This index organisation calculates the direct location of data record on disk.
Indices are based on the values being distributed uniformly across a range of buckets. The buckets to which a value is assigned are determined by the hash function.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/mastering_clickhouse-hash-file.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As it is the case for all Hash based organisations, the data stored is scattered, non-ordered, so cannot be compressed. Lots of space is not used and wasted.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sequential file index organisation:&lt;/strong&gt;
In this, the indices are based on a sorted ordering of the values. These are generally fast and a more traditional type of storing mechanism. These Ordered or Sequential file organizations might store the data in a dense or sparse format.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/mastering_clickhouse-sequential-file.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dense index: &lt;/strong&gt; 
We can see that the index has an entry for every first name in the table. If we want to look up a user with the first name “Arnaud,” then we perform a binary search on the index and read the location of the data. In contrast, a sparse index only has entries for some of the table rows.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sparse index:&lt;/strong&gt;&lt;br /&gt;
We can see that our sparse index only has 2 entries (one for each page). Now, if we want to find the row for “Arnaud,” we can perform a binary search on our index to find that it falls between “Anne” and “Mathieu”. After discovering those bounds, we go to the page starting with “Anne” and begin scanning for Arnaud’s row. Notice that the data is now sorted on the right side for this example. This sorting is a limitation of the sparse index. A sparse index requires ordered data; otherwise, the scanning step would be impossible.&lt;/p&gt;

&lt;p&gt;Dense indexes require more maintenance than sparse indexes at write-time. Since every row must have an entry, the database must maintain the index on inserts, updates, and deletes. Having an entry for every row also means that dense indexes will require more memory. The benefit of a dense index is that values can be quickly found with just a binary search. Dense indexes also do not impose any ordering requirements on the data.
Sparse indexes require less maintenance than dense indexes at write-time since they only contain a subset of the values. This lighter maintenance burden means that inserts, updates, and deletes will be faster. Having fewer entries also means that the index will use less memory. Finding data is slower since a scan across the page typically follows the binary search. Sparse indexes are also only an option when working with ordered data.&lt;/p&gt;

&lt;p&gt;Now. How those structures are used in clickhouse ?&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;internalCH&quot;&gt; 3. Clickhouse Internal Storage and Query Mechanism&lt;/h1&gt;

&lt;p&gt;Now lets take a practical example to illustrate the power of clickhouse.&lt;br /&gt;
Say we have the table below, containing 8.87 million entry:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;../assets/images/mastering_clickhouse-partitioned-table.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;If we execute the query below on that table:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-sql&quot; data-lang=&quot;sql&quot;&gt;&lt;span class=&quot;k&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Carrier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;toYear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FlightDate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Year&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;canceled&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;canceled&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;FROM&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;flights_ontime&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;WHERE&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Year&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2017&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;GROUP&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Carrier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;Year&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;HAVING&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cancelled&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;ORDER&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;BY&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Carrier&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;If we:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;read every row 				=&amp;gt; 	59GB  	%100&lt;/li&gt;
  &lt;li&gt;read only 3 columns 		=&amp;gt;	1,7GB	%3&lt;/li&gt;
  &lt;li&gt;read 3 cols compressed		=&amp;gt;	21MB	%0,035&lt;/li&gt;
  &lt;li&gt;read 3 cols comp 8 threads	=&amp;gt;	2,6MB	%0,0044&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In traditional relational database management systems, the primary index would contain one entry per table row. For our data set this would result in the primary index - often a B(+)-Tree data structure - containing 8.87 million entries. Such an index allows the fast location of specific rows, resulting in high efficiency for lookup queries and point updates. Searching an entry in a B(+)-Tree data structure has average time complexity of O(log2 n). For a table of 8.87 million rows, this means 23 steps are required to locate any index entry. This capability comes at a cost: additional disk and memory overheads and higher insertion costs when adding new rows to the table and entries to the index (and also sometimes rebalancing of the B-Tree).&lt;/p&gt;

&lt;p&gt;Considering the challenges associated with B-Tree indexes, table engines in ClickHouse utilise a different approach. The ClickHouse MergeTree Engine Family has been designed and optimized to handle massive data volumes. These tables are designed to receive millions of row inserts per second and store very large (100s of Petabytes) volumes of data. Data is quickly written to a table part by part, with rules applied for merging the parts in the background. In ClickHouse each part has its own primary index. When parts are merged, then the merged part’s primary indexes are also merged. At the very large scale that ClickHouse is designed for, it is paramount to be very disk and memory efficient. Therefore, instead of indexing every row, the primary index for a part has one index entry (known as a ‘&lt;strong&gt;mark&lt;/strong&gt;’) per group of rows (called ‘&lt;strong&gt;granule&lt;/strong&gt;’) - this technique is called sparse index.&lt;/p&gt;

&lt;p&gt;Sparse indexing is possible because ClickHouse is storing the rows for a part on disk ordered by the primary key column(s). Instead of directly locating single rows (like a B-Tree based index), the sparse primary index allows it to quickly (via a binary search over index entries) identify groups of rows that could possibly match the query. The located groups of potentially matching rows (granules) are then in parallel streamed into the ClickHouse engine in order to find the matches. This index design allows for the primary index to be small (it can, and must, completely fit into the main memory), whilst still significantly speeding up query execution times: especially for range queries that are typical in data analytics use cases.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/mastering_clickhouse-partitioning.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For data processing purposes, a table’s column values are logically divided into granules. A granule is the smallest indivisible data set that is streamed into ClickHouse for data processing. This means that instead of reading individual rows, ClickHouse is always reading (in a streaming fashion and in parallel) a whole group (granule) of rows.
The following diagram shows how the (column values of) 8.87 million rows of our table are organized into 1083 granules, as a result of the table’s DDL statement containing the setting index_granularity (set to its default value of 8192).
The primary index is created based on the granules shown in the diagram above. This index is an uncompressed flat array file (primary.idx), containing so-called numerical index marks starting at 0.
The diagram below shows that the index stores the primary key column values (the values marked in orange in the diagram above) for each first row for each granule. Or in other words: the primary index stores the primary key column values from each 8192nd row of the table (based on the physical row order defined by the primary key columns). For example
the first index entry (‘mark 0’ in the diagram below) is storing the key column values of the first row of granule 0 from the diagram above,
the second index entry (‘mark 1’ in the diagram below) is storing the key column values of the first row of granule 1 from the diagram above, and so on.&lt;/p&gt;

&lt;p&gt;When a query is filtering on a column that is part of a compound key and is the first key column, then ClickHouse is running the binary search algorithm over the key column’s index marks.&lt;/p&gt;

&lt;p&gt;As discussed above, ClickHouse is using its sparse primary index for quickly (via binary search) selecting granules that could possibly contain rows that match a query.
This is the first stage (granule selection) of ClickHouse query execution.
In the second stage (data reading), ClickHouse is locating the selected granules in order to stream all their rows into the ClickHouse engine in order to find the rows that are actually matching the query.&lt;/p&gt;

&lt;p&gt;In ClickHouse the physical locations of all granules for our table are stored in mark files. Similar to data files, there is one mark file per table column.
We have discussed how the primary index is a flat uncompressed array file (primary.idx), containing index marks that are numbered starting at 0.
Similarly, a mark file is also a flat uncompressed array file (*.mrk) containing marks that are numbered starting at 0.
Once ClickHouse has identified and selected the index mark for a granule that can possibly contain matching rows for a query, a positional array lookup can be performed in the mark files in order to obtain the physical locations of the granule.&lt;/p&gt;

&lt;p&gt;Each mark file entry for a specific column is storing two locations in the form of offsets:
The first offset (‘block_offset’ in the diagram above) is locating the block in the compressed column data file that contains the compressed version of the selected granule. This compressed block potentially contains a few compressed granules. The located compressed file block is uncompressed into the main memory on read.
The second offset (‘granule_offset’ in the diagram above) from the mark-file provides the location of the granule within the uncompressed block data.
Because compressed blocks contains data for multiple granules.&lt;/p&gt;

&lt;p&gt;All the 8192 rows belonging to the located uncompressed granule are then streamed into ClickHouse for further processing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/mastering_clickhouse-parts-internals.png&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Why MARK Files ?&lt;/h3&gt;
&lt;p&gt;Why does the primary index not directly contain the physical locations of the granules that are corresponding to index marks?&lt;br /&gt;
Because at that very large scale that ClickHouse is designed for, it is important to be very disk and memory efficient.&lt;br /&gt;
The primary index file needs to fit into the main memory.&lt;br /&gt;
For our example query, ClickHouse used the primary index and selected a single granule that can possibly contain rows matching our query. Only for that one granule does ClickHouse then need the physical locations in order to stream the corresponding rows for further processing.&lt;br /&gt;
Furthermore, this offset information is only needed for the UserID and URL columns.&lt;br /&gt;
Offset information is not needed for columns that are not used in the query e.g. the EventTime.&lt;br /&gt;
For our sample query, ClickHouse needs only the two physical location offsets for granule 176 in the UserID data file (UserID.bin) and the two physical location offsets for granule 176 in the URL data file (URL.bin).&lt;br /&gt;
The indirection provided by mark files avoids storing, directly within the primary index, entries for the physical locations of all 1083 granules for all three columns: thus avoiding having unnecessary (potentially unused) data in main memory.&lt;/p&gt;

&lt;hr /&gt;

&lt;h1&gt;Appendix: References&lt;/h1&gt;

&lt;p&gt;https://wikipedia.com&lt;br /&gt;
https://stackoverflow.com/questions/643694/what-is-the-difference-between-utf-8-and-unicode
https://dataschool.com/data-modeling-101/row-vs-column-oriented-databases/&lt;br /&gt;
https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/&lt;br /&gt;
https://clickhouse.com/docs/en/optimize/sparse-primary-indexes#an-index-design-for-massive-data-scales&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Özgün ÖZ</name>
        
        
      </author>

      

      
        <category term="development" />
      
        <category term="system" />
      

      
        <summary type="html">First article of the clickhouse 101 series. Lets deep dive into the underlying data structures and internal mechanisms of clickhouse.</summary>
      

      
      
    </entry>
  
</feed>
